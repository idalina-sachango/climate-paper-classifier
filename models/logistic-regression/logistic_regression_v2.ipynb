{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51f96207",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "import torchdata\n",
    "import portalocker\n",
    "import pandas as pd\n",
    "\n",
    "RANDOM_STATE = 30255\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72e94661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Renewable Energy Sources                       8117\n",
       "Geosciences                                     142\n",
       "Environmental Sciences                           98\n",
       "Energy Storage, Conversion, and Utilization      55\n",
       "Name: CLASS, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/preprocessed_data.csv')\n",
    "df = df[['CLASS', 'PREPROCESSED']]\n",
    "df = df.dropna()\n",
    "df['PREPROCESSED'] = df['PREPROCESSED'].str.replace(r'<[^<>]*>', '', regex=True) # drop HTML tags\n",
    "\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(df['CLASS'])\n",
    "df['LABEL'] = le.transform(df['CLASS'])\n",
    "df.head()\n",
    "\n",
    "display(df['CLASS'].value_counts())\n",
    "df = df[['LABEL', 'PREPROCESSED']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c9e352f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    8117\n",
       "2     142\n",
       "1      98\n",
       "0      55\n",
       "Name: LABEL, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['LABEL'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55acefb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_data(df, random_state):\n",
    "    \n",
    "    # Split the data into training, testing, and validation sets\n",
    "    train_data, test_data = train_test_split(df, test_size=0.3, random_state=random_state)\n",
    "    train_data, val_data = train_test_split(train_data, test_size=0.25, random_state=random_state)\n",
    "\n",
    "    # Convert the sets into iterable\n",
    "    train_iter = iter(train_data.values.tolist())\n",
    "    test_data = iter(test_data.values.tolist())\n",
    "    val_data = iter(val_data.values.tolist())\n",
    "    \n",
    "    return train_iter, test_data, val_data\n",
    "\n",
    "train_iter, test_data, val_data = split_data(df, RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00c8d29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "def yield_tokens(train_iter):\n",
    "    for _, text in train_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "    \n",
    "vocab = build_vocab_from_iterator(\n",
    "    yield_tokens(train_iter), specials=[\"<unk>\"], min_freq=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0024149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1: The number of words in the Vocab object is 4666.\n",
      "Task 2: The index of the word 'energy' is 4.\n",
      "Task 3: The word at index 500 is 'strong'.\n",
      "Task 4: The index of the word '<unk>' is 0. Resetting default index to this value.\n"
     ]
    }
   ],
   "source": [
    "# Task 1\n",
    "print(f\"Task 1: The number of words in the Vocab object is {len(vocab)}.\")\n",
    "\n",
    "# # Task 2\n",
    "stoi_dict = vocab.get_stoi()\n",
    "word = \"energy\"\n",
    "print(f\"Task 2: The index of the word '{word}' is {stoi_dict[word]}.\")\n",
    "\n",
    "# # Task 3\n",
    "itos_dict = vocab.get_itos()\n",
    "idx = 500\n",
    "print(f\"Task 3: The word at index 500 is '{itos_dict[idx]}'.\")\n",
    "\n",
    "# # Task 4:\n",
    "word = \"<unk>\"\n",
    "print(f\"Task 4: The index of the word '{word}' is {stoi_dict[word]}. Resetting default index to this value.\")\n",
    "vocab.set_default_index(stoi_dict[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e204bcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from collections import Counter\n",
    "\n",
    "def collate_into_bow(batch):\n",
    "    '''\n",
    "    Generates a tensor of batch labels and a tensor of relative token frequencies.\n",
    "    \n",
    "    arg:\n",
    "    - batch: List of tuples, first element of tuple\n",
    "        is a label, second element is text\n",
    "    - assumes that Vocab object is created\n",
    "    tr\n",
    "    Returns:\n",
    "    - Tensor (1D; same length as batch) showing text labels (indexed to 0)\n",
    "    - Tensor (2D; rows are the length of batch, columns are length of Vocab object)\n",
    "        showing the relative frequency of each token within the text\n",
    "    '''\n",
    "    # get tensor dimensions\n",
    "    k = len(batch)\n",
    "    m = len(vocab)\n",
    "    \n",
    "    # initialize empty tensors\n",
    "    tensor_labels = torch.zeros((k, ), dtype=torch.int64)\n",
    "    tensor_rf = torch.zeros((k, m))\n",
    "\n",
    "    # iterate over batch\n",
    "    for idx, (label, txt) in enumerate(batch):\n",
    "\n",
    "        # get individual tokens\n",
    "        txt_split = txt.split(\" \")\n",
    "\n",
    "        # get indices for each token\n",
    "        txt_indices = vocab.lookup_indices(txt_split)\n",
    "\n",
    "        # get frequencies for eacch token\n",
    "        idx_freq_dict = dict(Counter(txt_indices))\n",
    "\n",
    "        # update tensor with frequency of each token\n",
    "        tensor_rf[idx, list(idx_freq_dict.keys())] += torch.tensor(list(idx_freq_dict.values()))\n",
    "\n",
    "    # normalize so that rows sum to 1\n",
    "    tensor_row_sum = tensor_rf.sum(dim=1, keepdim=True)\n",
    "    tensor_rf = tensor_rf / tensor_row_sum\n",
    "    \n",
    "    return tensor_labels, tensor_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb2692bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import IterableDataset\n",
    "\n",
    "class MyIterableDataset(IterableDataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def __iter__(self):\n",
    "        # Return an iterator over your data\n",
    "        return iter(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff24f328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([16]) torch.Size([16, 4666])\n",
      "1 torch.Size([16]) torch.Size([16, 4666])\n",
      "2 torch.Size([16]) torch.Size([16, 4666])\n",
      "3 torch.Size([16]) torch.Size([16, 4666])\n",
      "4 torch.Size([16]) torch.Size([16, 4666])\n"
     ]
    }
   ],
   "source": [
    "train_iter, test_data, val_data = split_data(df, RANDOM_STATE)\n",
    "dataloader = DataLoader(MyIterableDataset(train_iter), batch_size=BATCH_SIZE, shuffle=False, \n",
    "                        collate_fn=collate_into_bow)\n",
    "for idx, (lt, tt) in enumerate(dataloader):\n",
    "    print(idx, lt.shape, tt.shape)\n",
    "    if idx == 4: \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "980da068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a BoWClassifier class with one single linear layer\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BoWClassifier(nn.Module):\n",
    "    def __init__(self, num_labels, vocab_size):\n",
    "        super(BoWClassifier, self).__init__()\n",
    "        \n",
    "        # create affine map\n",
    "        self.linear = nn.Linear(vocab_size, num_labels)\n",
    "        \n",
    "    def forward(self, bow_vec):\n",
    "        # single linear layer\n",
    "        return F.log_softmax(self.linear(bow_vec), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eeb102d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, _, _ = split_data(df, RANDOM_STATE)\n",
    "num_labels = len(set([label for (label, text) in train_data]))\n",
    "vocab_size = len(vocab)\n",
    "model = BoWClassifier(num_labels, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eda828c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "loss_function = torch.nn.NLLLoss()\n",
    "\n",
    "def train_an_epoch(dataloader, optimizer):\n",
    "    model.train() # Sets the module in training mode.\n",
    "    log_interval = 500\n",
    "\n",
    "    for idx, (label, text) in enumerate(dataloader):\n",
    "        model.zero_grad()\n",
    "        log_probs = model(text)\n",
    "        loss = loss_function(log_probs, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            print(f'At iteration {idx} the loss is {loss:.3f}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03771233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to compute accuracy\n",
    "\n",
    "def get_accuracy(dataloader):\n",
    "    '''\n",
    "    Compute accuracy rate of model. Generate\n",
    "    model predictions, compare to true labels,\n",
    "    and compute accuracy.\n",
    "    \n",
    "    args:\n",
    "    - dataloader (object)\n",
    "    \n",
    "    Returns: An accuracy rate (float)\n",
    "    '''\n",
    "    \n",
    "    # switch to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # initialize counters\n",
    "    correct_count = 0.0\n",
    "    example_count = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        i = 0\n",
    "        # unpack dataloader\n",
    "        for tl, tt in dataloader:\n",
    "            \n",
    "            # get the size of the batch\n",
    "            example_count += tl.shape[0]\n",
    "\n",
    "            # get predicted values (label with highest probability)\n",
    "            model_result = model(tt)\n",
    "            tensor_pred = model_result.argmax(dim=1) \n",
    "\n",
    "            # count how often predictions match true labels\n",
    "            correct_count_batch = (tensor_pred == tl).sum().item()\n",
    "            correct_count += correct_count_batch\n",
    "            \n",
    "            i += 1\n",
    "    \n",
    "    if example_count == 0:\n",
    "        print(\"correct_count:\", correct_count, \"iter number:\", i)\n",
    "    return correct_count / example_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02465bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(df, RANDOM_STATE, collate_fn):\n",
    "    train_iter, test_data, val_data = split_data(df, RANDOM_STATE)\n",
    "    train_dataloader = DataLoader(MyIterableDataset(train_iter), batch_size=BATCH_SIZE, shuffle=False, \n",
    "                        collate_fn=collate_fn)\n",
    "    test_dataloader = DataLoader(MyIterableDataset(test_data), batch_size=BATCH_SIZE, \n",
    "                        collate_fn=collate_fn)\n",
    "    val_dataloader = DataLoader(MyIterableDataset(val_data), batch_size=BATCH_SIZE, \n",
    "                        collate_fn=collate_fn)\n",
    "    \n",
    "    return train_dataloader, test_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "582a2db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--\n",
      "Time taken: 3.014.\n",
      "After epoch 1 the train accuracy is 1.000.\n",
      "After epoch 1 the test accuracy is 1.000.\n",
      "After epoch 1 the validation accuracy is 1.000.\n",
      "--\n",
      "--\n",
      "Time taken: 2.723.\n",
      "After epoch 2 the train accuracy is 1.000.\n",
      "After epoch 2 the test accuracy is 1.000.\n",
      "After epoch 2 the validation accuracy is 1.000.\n",
      "--\n",
      "--\n",
      "Time taken: 2.704.\n",
      "After epoch 3 the train accuracy is 1.000.\n",
      "After epoch 3 the test accuracy is 1.000.\n",
      "After epoch 3 the validation accuracy is 1.000.\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "# TRAINING\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "EPOCHS = 3 # epoch\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=3)\n",
    "\n",
    "test_accuracies=[]\n",
    "train_accuracies=[] # added\n",
    "valid_accuracies=[]\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    train_dataloader, _, _ = get_dataloaders(df, RANDOM_STATE, collate_into_bow)\n",
    "    train_an_epoch(train_dataloader, optimizer)\n",
    "    \n",
    "    train_dataloader, test_dataloader, val_dataloader = get_dataloaders(df, RANDOM_STATE, collate_into_bow)\n",
    "    train_accuracy = get_accuracy(train_dataloader) # added\n",
    "    test_accuracy = get_accuracy(test_dataloader)\n",
    "    val_accuracy = get_accuracy(val_dataloader)\n",
    "\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    train_accuracies.append(test_accuracy)\n",
    "    valid_accuracies.append(val_accuracy)\n",
    "    time_taken = time.time() - epoch_start_time\n",
    "    print('--')\n",
    "    print(f'Time taken: {time_taken:.3f}.')\n",
    "    print(f'After epoch {epoch} the train accuracy is {train_accuracy:.3f}.')\n",
    "    print(f'After epoch {epoch} the test accuracy is {test_accuracy:.3f}.')\n",
    "    print(f'After epoch {epoch} the validation accuracy is {val_accuracy:.3f}.')\n",
    "    print('--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b66f831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add pre-trained embeddings\n",
    "\n",
    "from itertools import combinations\n",
    "from torchtext.vocab import GloVe\n",
    "\n",
    "# Save GloVe data in a cache\n",
    "VECTOR_CACHE_DIR = '../.vector_cache'\n",
    "\n",
    "glove = GloVe(name='6B', cache = VECTOR_CACHE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e94465d",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_size = glove.dim\n",
    "\n",
    "def collate_into_cbow(batch):\n",
    "    '''\n",
    "    Generates a tensor of batch labels and a tensor of mean GloVe embeddings for each token.\n",
    "    \n",
    "    arg:\n",
    "    - batch: List of tuples, first element of tuple is a label, second element is text\n",
    "    - assumes that GloVe object is created\n",
    "    \n",
    "    Returns:\n",
    "    - Tensor (1D; same length as batch) showing text labels (indexed to 0)\n",
    "    - Tensor (2D; rows are the length of batch, columns are size of GloVe embeddings)\n",
    "        showing the average of GloVe embeddings for each text\n",
    "    '''\n",
    "    # get tensor dimensions\n",
    "    k = len(batch)\n",
    "    m = glove.dim\n",
    "\n",
    "    # initialize empty tensors\n",
    "    tensor_labels = torch.zeros((k, ), dtype=torch.int64)\n",
    "    tensor_glove = torch.zeros((k, m))\n",
    "\n",
    "    # iterate over batch\n",
    "    for idx, (label, txt) in enumerate(batch):\n",
    "\n",
    "        # get individual tokens\n",
    "        txt_split = txt.split(\" \")\n",
    "\n",
    "        # get GloVe embeddings for each token\n",
    "        txt_embedding = glove.get_vecs_by_tokens(txt_split)\n",
    "        \n",
    "        # update tensor with average of GloVe embeddings\n",
    "        tensor_glove[idx, :] = txt_embedding.mean(dim=0)\n",
    "\n",
    "    return tensor_labels, tensor_glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6fd44af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--\n",
      "Time taken: 8.869.\n",
      "After epoch 1 the train accuracy is 0.346.\n",
      "After epoch 1 the test accuracy is 0.342.\n",
      "After epoch 1 the validation accuracy is 0.359.\n",
      "--\n",
      "--\n",
      "Time taken: 8.473.\n",
      "After epoch 2 the train accuracy is 0.346.\n",
      "After epoch 2 the test accuracy is 0.342.\n",
      "After epoch 2 the validation accuracy is 0.359.\n",
      "--\n",
      "--\n",
      "Time taken: 11.211.\n",
      "After epoch 3 the train accuracy is 0.346.\n",
      "After epoch 3 the test accuracy is 0.342.\n",
      "After epoch 3 the validation accuracy is 0.359.\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 3 # epoch\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=3)\n",
    "model = BoWClassifier(num_labels, glove_size)\n",
    "\n",
    "val_accuracies=[]\n",
    "train_accuracies=[] # added\n",
    "test_accuracies=[] # added\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    train_dataloader, _, _ = get_dataloaders(df, RANDOM_STATE, collate_into_cbow)\n",
    "    train_an_epoch(train_dataloader, optimizer)\n",
    "    \n",
    "    train_dataloader, test_dataloader, val_dataloader = get_dataloaders(df, RANDOM_STATE, collate_into_cbow)\n",
    "    train_accuracy = get_accuracy(train_dataloader) # added\n",
    "    test_accuracy = get_accuracy(test_dataloader)\n",
    "    val_accuracy = get_accuracy(val_dataloader)\n",
    "\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    train_accuracies.append(test_accuracy)\n",
    "    valid_accuracies.append(val_accuracy)\n",
    "    time_taken = time.time() - epoch_start_time\n",
    "    print('--')\n",
    "    print(f'Time taken: {time_taken:.3f}.')\n",
    "    print(f'After epoch {epoch} the train accuracy is {train_accuracy:.3f}.')\n",
    "    print(f'After epoch {epoch} the test accuracy is {test_accuracy:.3f}.')\n",
    "    print(f'After epoch {epoch} the validation accuracy is {val_accuracy:.3f}.')\n",
    "    print('--')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capp30255",
   "language": "python",
   "name": "capp30255"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
